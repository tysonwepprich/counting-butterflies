---
title: "Validation of GAM + MM methods"
author: "Tyson Wepprich"
date: "October 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# packages to load
library(mgcv)
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(lubridate)
library(mclust)
library(mixsmsn)
# devtools::install_github("hadley/multidplyr")
library(multidplyr)

```

Having developed new methods for modeling butterfly populations from citizen science Pollard transects for my dissertation, I need to verify that they work!

Here's the approach:
1. Simulate counts with varying species parameters.
2. Fit with GAM for whole flight curve.
3. Use mixture model for generation specific phenology/abundance.
4. Use approach on Ohio butterfly data with additional covariates.

Use Calabrese 2012 to simulate abundance curves based on Zonneveld 1991 model.
```{r parametric_abundance_curves}

sig <- 1 # shape parameter, left skew if > 1, right skew if <1
mu <- 10 # location of peak emergence
t <- seq(1, 30, 1) # timestep
beta <- 1 # dispersion of emergence curve, scale parameter
alpha <- .3 # death rate per time

Abund_Curve <- function(t = t, alpha = .3, beta = 1, mu = 10, sig = 1){
  
  z <- exp((t - mu) / beta) / (1 + exp((t - mu) / beta))
  a <- 1 + alpha * beta
  b <- sig - alpha * beta
  
  # incomplete beta
  ibeta <- function(z, a, b){ pbeta(z, a, b) * beta(a, b) }
  
  e <- (sig * (exp((t - mu)/beta)))/(beta * (1 + exp((t - mu)/beta)))^(sig + 1)
  e <- e / sum(e)
  y <- alpha * sig * exp(-alpha * (t - mu)) * ibeta(z, a, b)
  y <- y / sum(y)
  return(data.frame(t, y, e))
}

test <- Abund_Curve(t, alpha, beta, mu, sig)
# gdd scale?
test <- Abund_Curve(t = seq(0, 2000, 50), alpha = .01, beta = 40, mu = 1000, sig = 1)


plt <- ggplot(test, aes(x = t, y = y)) + 
  geom_point() +
  geom_path(aes(y = e), color = "blue")
plt






```

GDD across gradient
```{r site_gdd_variation}
sites <- read.csv("data/OHsites_reconciled_update2016.csv")
sites$SiteID <- formatC(as.numeric(sites$Name), width = 3, format = "d", flag = "0")
sites$Name <- NULL
gdd <- readRDS("data/dailyDD.rds")
# gdd <- readRDS("../ohiogdd/dailyDD.rds")


# sitesumm <- gdd %>% 
#   group_by(SiteID, year(SiteDate)) %>% 
#   summarise(AvailGDD = sum(degday1030))

gdd <- left_join(gdd, sites) %>% 
  dplyr::select(SiteID, SiteDate, degday530, lat, lon, maxT) %>% 
  mutate(Year = year(SiteDate),
         DOY = yday(SiteDate)) %>% 
  group_by(SiteID, Year) %>% 
  arrange(DOY) %>% 
  mutate(AccumDD = cumsum(degday530))

```



Simulation

```{r simulate_butterfly_counts}
# parameters
nsite <- 25 # up to 140 OH sites
nyear <- 10 # up to 35 years in Daymet data
nsurv <- 30
surv_missing <- .2 # remove surveys for each site, based on data distribution
site_missing <- .2 # turnover across years

ngen <- 3
# volt_flex <- "Y"
gen_ddreq <- 600
peak_sd <- 10
death_rate <- .6 # on weekly rate

# site total population dispersion
negbin_mu <- 100
negbin_disp <- 1
# detection probability parameters (logit scale)
detprob_b0 <- -7 
detprob_b1 <- .6
detprob_b2 <- -.01

# # nonlinear det prob
x <- seq(-5, 45, .1)
y <- plogis(-7 + x * .6 + -.01 * x^2)
plot(x, y)

# subroutines

# lat/lon of sites with variation in degree-days available on simple N/S gradient
# randomly select sites and years to use their coordinates and historical gdd accumulation
sites <- sample(x = unique(gdd$SiteID), size = nsite, replace = FALSE)
years <- sample(x = unique(gdd$Year), size = nyear, replace = FALSE)
dddat <- gdd %>% 
  filter(SiteID %in% sites, Year %in% years) %>% 
  filter(DOY %in% (seq(77, 308, 7) + sample.int(n=6, size=34, replace=TRUE)))

# voltinism varying by site

# simulate phenology for each generation and combine
dflist <- list()
for (g in 1:ngen){
  df <- Abund_Curve(t = dddat$AccumDD/100, alpha = death_rate, 
                                beta = g * peak_sd / 100, mu = g * gen_ddreq / 100, sig = 1)
  df <- cbind(data.frame(dddat), df)
  df$Gen <- g
  dflist[[g]] <- df
}
dfall <- bind_rows(dflist) %>% 
  arrange(SiteID, SiteDate)


plt <- ggplot(dfall, aes(x = AccumDD, y = y, group = interaction(SiteID, Gen), color = Gen)) +
  geom_line() +
  facet_wrap(~Year)
plt

# dd <- dfall %>% dplyr::filter(SiteID %in% unique(.$SiteID)[seq(1, nsite, length.out = 5)])
# plt <- ggplot(dd, aes(x = AccumDD, y = y, group = Year, color = Gen)) +
#   geom_line() +
#   facet_wrap(~SiteID)
# plt
# counting process
counts <- dfall %>% 
  group_by(SiteID, SiteDate, lat, lon, Year, DOY, AccumDD, maxT) %>% 
  summarise(RelProp = sum(y * Gen),
            DP = plogis(detprob_b0 + detprob_b1 * maxT[1] + detprob_b2 * maxT[1]^2)) %>% 
  group_by(SiteID, Year) %>% 
  mutate(RelProp = RelProp / sum(RelProp),
         M = rnbinom(1, mu = negbin_mu, size = negbin_disp),
         N = rpois(length(RelProp), lambda = RelProp * M),
         Y = rbinom(length(N), size = N, prob = DP)) %>% 
  data.frame()

# plt <- ggplot(counts, aes(x = AccumDD, y = Y, group = SiteID)) +
#   geom_point() +
#   facet_wrap(~Year)
# plt

# mixture models for subset
test_sy <- counts %>% filter(SiteID == "174", Year == unique(Year)[5])
test_s <- counts %>% filter(SiteID == "003")
test_y <- counts %>% filter(Year == unique(Year)[5])

plt <- ggplot(test_sy, aes(x = AccumDD, y = Y, group = Year, color = SiteID)) +
  geom_point() 
plt


# for now just output estimated mu
CompareMixMods <- function(dat, mvmin, mvmax){
  dd <- dat$AccumDD
  y <- dat$Y
  dd_dist <- rep(dd, y)
  # for output of classification of each observation
  # outdf <- data.frame(SiteID = rep(dat$SiteID, y), SiteDate = rep(dat$SiteDate, y))
  out <- list()
  
  # NORMAL
  # get initial vals from search for best num gen
  mod_sk <- smsn.search(dd_dist, nu = 5, g.min = mvmin, g.max = mvmax, family = "Skew.normal", kmeans.param = list(n.start = 5))
  
  init.pii <- mod_sk$best.model$pii
  init.pii[length(init.pii)] <- 1 - sum(init.pii[1:(length(init.pii) - 1)])
  
  # # giving starting values doesn't save much time?
  # mod_final_sk <- smsn.mix(dd_dist, nu = mod_sk$best.model$nu, 
  #                          mu = mod_sk$best.model$mu,
  #                          sigma2 = mod_sk$best.model$sigma2, 
  #                          shape = mod_sk$best.model$shape,
  #                          pii = init.pii, g = length(mod_sk$best.model$mu), get.init = FALSE, group = TRUE, obs.prob = TRUE, family = "Skew.normal", calc.im = TRUE)
  
  mod_final_sk <- smsn.mix(dd_dist, nu = 5, g = length(mod_sk$best.model$mu), get.init = TRUE, family = "Skew.normal", calc.im = TRUE, obs.prob = TRUE, kmeans.param = list(n.start = 5))
  
  out[[1]] <- data.frame(model = "smsn_sknorm", G = 1:length(mod_final_sk$mu), mu = sort(mod_final_sk$mu), w = mod_final_sk$pii[rank(mod_final_sk$mu)])
  
  # Normal het from smsn
  mod_norm <- smsn.search(dd_dist, nu = 5, g.min = mvmin, g.max = mvmax, family = "Normal", kmeans.param = list(n.start = 5))
  
  init.pii <- mod_norm$best.model$pii
  init.pii[length(init.pii)] <- 1 - sum(init.pii[1:(length(init.pii) - 1)])
  
  mod_final_norm <- smsn.mix(dd_dist, nu = 5, g = length(mod_norm$best.model$mu), get.init = TRUE, family = "Normal", calc.im = TRUE, obs.prob = TRUE, kmeans.param = list(n.start = 5))
  
  out[[2]] <- data.frame(model = "smsn_norm", G = 1:length(mod_final_norm$mu), mu = sort(mod_final_norm$mu), w = mod_final_norm$pii[rank(mod_final_norm$mu)]) 
  
  # T DIST
  # get initial vals from search for best num gen
  mod_skt <- smsn.search(dd_dist, nu = 5, g.min = mvmin, g.max = mvmax, family = "Skew.t", kmeans.param = list(n.start = 5))
  
  init.pii <- mod_skt$best.model$pii
  init.pii[length(init.pii)] <- 1 - sum(init.pii[1:(length(init.pii) - 1)])
  
  mod_final_skt <- smsn.mix(dd_dist, nu = 5, g = length(mod_skt$best.model$mu), get.init = TRUE, family = "Skew.t", calc.im = TRUE, obs.prob = TRUE, kmeans.param = list(n.start = 5))
  out[[3]] <- data.frame(model = "smsn_skt", G = 1:length(mod_final_skt$mu), mu = sort(mod_final_skt$mu), w = mod_final_skt$pii[rank(mod_final_skt$mu)])
  
  # T het from smsn
  mod_t <- smsn.search(dd_dist, nu = 5, g.min = mvmin, g.max = mvmax, family = "t", kmeans.param = list(n.start = 5))
  
  init.pii <- mod_t$best.model$pii
  init.pii[length(init.pii)] <- 1 - sum(init.pii[1:(length(init.pii) - 1)])
  
  mod_final_t <- smsn.mix(dd_dist, nu = 5, g = length(mod_t$best.model$mu), get.init = TRUE, family = "t", calc.im = TRUE, obs.prob = TRUE, kmeans.param = list(n.start = 5))
  out[[4]] <- data.frame(model = "smsn_t", G = 1:length(mod_final_t$mu), mu = sort(mod_final_t$mu), w = mod_final_t$pii[rank(mod_final_t$mu)])
  
  
  mod_mc_hom <- try(Mclust(dd_dist, G=c(mvmin:mvmax), modelNames = "E"), silent = TRUE)
  mod_mc_het <- try(Mclust(dd_dist, G=c(mvmin:mvmax), modelNames = "V"), silent = TRUE)
  
  out[[5]] <- data.frame(model = "mc_hom", G = 1:length(mod_mc_hom$parameters$pro), mu = sort(mod_mc_hom$parameters$mean), w = mod_mc_hom$parameters$pro[rank(mod_mc_hom$parameters$mean)])
  out[[6]] <- data.frame(model = "mc_het", G = 1:length(mod_mc_het$parameters$pro), mu = sort(mod_mc_het$parameters$mean), w = mod_mc_het$parameters$pro[rank(mod_mc_het$parameters$mean)])
  # get results
  result <- bind_rows(out)
  return(result)
}

a <- CompareMixMods(test$y, 1, 2)

test <- counts %>% 
  group_by(Year) %>% 
  do(CompareMixMods(., mvmin = 2, mvmax = 4))

test1 <- CompareMixMods(counts, 2, 4)
test1b <- CompareMixMods(counts, 2, 3)
test1c <- CompareMixMods(counts, 2, 4)
test1d <- CompareMixMods(counts, 2, 3)

# issues with above:
# model selection keeps choosing 4 generations, instead of true 3
# often with lots of overlap between 2 modes for last generation
# sometimes will run same skew t model and get different best results
# haven't tried more sites yet
# haven't rigorously tested site x year vs year vs all grouping and disaggregation

test2 <- test %>% 
  group_by(model, G) %>% 
  summarise(meanmu = mean(mu),
            meanw = mean(w))


counts$SiteID <- as.factor(counts$SiteID)

modnb <- gam(Y ~ 
                         # s(zlistlength)+
                         s(maxT)+
                         # s(zduration)+
                         # s(SiteYear, cumdegday, bs = "fs", k = 5, m = 1) +
                          s(AccumDD, bs = "cc", k = 20) +
                         # te(lat, lon, AccumDD, bs = c("tp", "cc"), k = c(5, 20), d = c(2, 1)) +
                         s(SiteID, bs = "re"),
                         # s(Ordinal, bs = "cc", k = 10),
                       family = nb(theta = NULL, link = "log"),
                       # family = poisson(link = "log"),
                       data = counts,
                       method = "REML", 
                       optimizer = c("outer", "newton"), 
                       # gamma = 1.4, 
                       control = list(maxit = 500))

  counts$Y <- predict(modnb, newdata = counts, type = "response")

library(MASS)  
  Xp <- predict.gam(object = modnb, newdata = counts, type="lpmatrix") ## map coefs to fitted curves
  beta <- coef(modnb)
  Vb   <- vcov(modnb) ## posterior mean and cov of coefs
  n <- 1 # choose number of simulations
  # set.seed(10)
  mrand <- mvrnorm(n, beta, Vb) ## simulate n rep coef vectors from posterior
  ilink <- family(modnb)$linkinv
  # linklppreds <- Xp %*% t(mrand)
    linklppreds <- Xp %*% mrand
  nbpreds <- apply(X = linklppreds, 
                   MARGIN = 1, 
                   FUN = function(x){
                     temp <- sort(x)
                     bounds <- quantile(1:n, probs = c(0.025, 0.975))
                     x <- temp[bounds[1]:bounds[2]]
                     x <- ilink(x)
                     x <- rnbinom(n = length(x),
                                  mu = x,
                                  size = modnb$family$getTheta(TRUE))
                     return(x)
                   })
  
    counts$Y <- x

    
    test1c <- CompareMixMods(counts, 2, 3)

```

What's the test worth publishing?
Simulation to compare performance of normal vs skewed mixture models.
Data degradation to see how sites/years should be grouped to keep model estimates of generation size/phenology accurate. Whether GAM is needed to smooth before mixture models.
1. Simulate statewide population over years.
  - Lifespan and emergence spread determining skew and overlap.
  - Beginning and end of season curves. Eventually add frost boundaries.
  - Voltinism and GDD requirements.
  - Varying population size by generation.
  - Dispersion of negative binomial in survey counts (or poisson).
  - Detection probability with single covariate.
2. Different % of data removed. Even or good/bad sites.
3. Cluster raw data into generations, by single site/year, by site, by year, all.
4. GAM by single site/year, by site, by year, all. Then cluster predictions with model uncertainty included.
5. Compare actual/estimated N, mu, sd of each generation.
6. Discuss physiological vs ordinal time scales? End of year problems.





```{r params}
# parameters
nsite <- c(2, 4, 8, 16, 32, 64) # up to 140 OH sites
nyear <- c(2, 4, 8, 16) # up to 35 years in Daymet data
nsurv <- 30
surv_missing <- c(0, .25, .5) # remove surveys for each site, based on data distribution
# site_missing <- .2 # turnover across years
group_struct <- c("all", "site", "year", "siteyear")
gam_smooth <- c("yes", "no")

ngen <- c(1:4)
gen_size <- c("equal", "inc", "dec")
# volt_flex <- "Y"
# gen_ddreq <- 600 # depends on ngen
peak_sd <- c(10, 30) # low and high overlap?
death_rate <- c(.6, .8) # on weekly rate

# site total population dispersion
negbin_mu <- 100
negbin_disp <- 1
# detection probability parameters (logit scale)
detprob_b0 <- c(-10, -7) 
detprob_b1 <- .6
detprob_b2 <- -.01

# # nonlinear det prob
# x <- seq(-5, 45, .1)
# y <- plogis(-7 + x * .6 + -.01 * x^2)
# plot(x, y)

params <- list(nsite = nsite, nyear = nyear, nsurv = nsurv, surv_missing = surv_missing, group_struct = group_struct, gam_smooth = gam_smooth, ngen = ngen, gen_size = gen_size, peak_sd = peak_sd, death_rate = death_rate, negbin_mu = negbin_mu, negbin_disp = negbin_disp, detprob_b0 = detprob_b0, detprob_b1 = detprob_b1, detprob_b2 = detprob_b2)
params <- expand.grid(params)

```

What should be measured for simulated curves? To be compared with mixmod results later.

```{r metrics}
data <- params[901, ]
df <- Abund_Curve(t = seq(1, 30, .1), alpha = data$death_rate, 
                      beta = gen_relsprd[g] * data$peak_sd / 100, mu = gen_ddreq[g] / 100, sig = 1)

df$cdf <- cumsum(df$y) / sum(df$y)
curve_mean <- weighted.mean(df$t, df$y)
curve_med <- df$t[which(abs(df$cdf - .5) == min(abs(df$cdf - .5)))]
curve_max <- df$t[which(df$y == max(df$y))]

dat <- Simulate_Truth(params[2261, ])
grpdat <- dat %>% 
  group_by(t) %>% 
  summarise(ally = sum(y))

pp <- ggplot(dat, aes(x = t, y = y)) + 
  geom_point(aes(color = Gen)) +
  geom_point(data = grpdat, aes(x = t, y = ally))
pp
```



For each row of parameters, simulate true values and counts.

```{r simulate}
Simulate_Truth <- function(data){
  set.seed(111) # use for all rows?
  
  # voltinism varying by site
  if(data$ngen == 1){
    gen_ddreq <- 800
    gen_relsize = 1
    gen_relsprd = 1
  }else{
    ngen <- data$ngen
    gen_ddreq <- seq(600, 1600, length.out = ngen)
    gen_relsize <- switch(data$gen_size, 
                          "equal" = rep(1, ngen),
                          "inc" = seq(1, 2, length.out = ngen),
                          "dec" = seq(2, 1, length.out = ngen))
    gen_relsprd <- seq(1, 2, length.out = ngen)
  }
  # simulate phenology for each generation and combine
  dflist <- list()
  for (g in 1:data$ngen){
    df <- Abund_Curve(t = seq(0, 3000, 1)/100, alpha = data$death_rate, 
                      beta = gen_relsprd[g] * data$peak_sd / 100, mu = gen_ddreq[g] / 100, sig = 1)
    df$Gen <- g
    dflist[[g]] <- df
  }
  dfall <- bind_rows(dflist)
  return(dfall)
}


Simulate_Counts <- function(data, gdd){
  set.seed(111) # use for all rows?
  # randomly select sites and years to use their coordinates and historical gdd accumulation
  sites <- sample(x = unique(gdd$SiteID), size = data$nsite, replace = FALSE)
  years <- sample(x = unique(gdd$Year), size = data$nyear, replace = FALSE)
  dddat <- gdd %>% 
    filter(SiteID %in% sites, Year %in% years) %>% 
    filter(DOY %in% (seq(90, 293, 7) + sample.int(n=6, size=30, replace=TRUE))) %>% 
    sample_frac(size = 1 - data$surv_missing)
  
  # voltinism varying by site
  if(data$ngen == 1){
    gen_ddreq <- 800
    gen_relsize = 1
    gen_relsprd = 1
  }else{
    ngen <- data$ngen
    gen_ddreq <- seq(600, 1600, length.out = ngen)
    gen_relsize <- switch(data$gen_size, 
                          "equal" = rep(1, ngen),
                          "inc" = seq(1, 2, length.out = ngen),
                          "dec" = seq(2, 1, length.out = ngen))
    gen_relsprd <- seq(1, 2, length.out = ngen)
  }
  # simulate phenology for each generation and combine
  dflist <- list()
  for (g in 1:data$ngen){
    df <- Abund_Curve(t = dddat$AccumDD/100, alpha = data$death_rate, 
                      beta = gen_relsprd[g] * data$peak_sd / 100, mu = gen_ddreq[g] / 100, sig = 1)
    df <- cbind(data.frame(dddat), df)
    df$Gen <- g
    dflist[[g]] <- df
  }
  dfall <- bind_rows(dflist) %>% 
    arrange(SiteID, SiteDate)
  
  # counting process
  counts <- dfall %>% 
    group_by(SiteID, SiteDate, lat, lon, Year, DOY, AccumDD, maxT) %>% 
    summarise(RelProp = sum(y * gen_relsize[Gen]),
              DP = plogis(data$detprob_b0 + data$detprob_b1 * maxT[1] + data$detprob_b2 * maxT[1]^2)) %>% 
    group_by(SiteID, Year) %>% 
    mutate(RelProp = RelProp / sum(RelProp),
           M = rnbinom(1, mu = data$negbin_mu, size = data$negbin_disp),
           N = rpois(length(RelProp), lambda = RelProp * M),
           Y = rbinom(length(N), size = N, prob = DP)) %>% 
    data.frame()
  return(counts)
}


Abund_Curve <- function(t = t, alpha = .3, beta = 1, mu = 10, sig = 1){
  
  z <- exp((t - mu) / beta) / (1 + exp((t - mu) / beta))
  a <- 1 + alpha * beta
  b <- sig - alpha * beta
  
  # incomplete beta
  ibeta <- function(z, a, b){ pbeta(z, a, b) * beta(a, b) }
  
  e <- (sig * (exp((t - mu)/beta)))/(beta * (1 + exp((t - mu)/beta)))^(sig + 1)
  e <- e / sum(e)
  y <- alpha * sig * exp(-alpha * (t - mu)) * ibeta(z, a, b)
  y <- y / sum(y)
  return(data.frame(t, y, e))
}

```

Try out the process in serial. 
1. row of params 
2a. simulate truth, all sites/years have same physiological traits (could change to random effects)
2b. mixture models of truth for comparison of distribution parameters? 
3. simulate counts
4a. mixture models of counts
4b. mixture models of counts after smoothing
5. assign counts to generation classification
6. Get N and mu for each site x year x generation with another round of mixture models?
7. Scores to assess fit: RMSE/R2 for N, phenology metrics (mean, median, mode, quantiles, skew distribution parameters?)

```{r serial attempt}

counts <- Simulate_Counts(data = params[1, ], gdd = gdd)

results <- counts %>% 
  group_by(Year) %>% 
  do(mixmods = CompareMixMods(dat = ., mvmin = 1, mvmax = 2))

```



```{r parallel attempt}
# test <- Simulate_Counts(params[1,])

# # this works, might want to use do instead of map for parallel dplyr processing
# data <- params[1:5,] %>% 
#   mutate(sim = row_number()) %>%
#   group_by(sim) %>%
#   # nest() %>%
#   # mutate(counts = purrr::map(data, ~Simulate_Counts(data = ., gdd = gdd)))
#   do(counts = Simulate_Counts(data = ., gdd = gdd))

# 
# test3 <- test2 %>% 
#   do(mixes = CompareMixMods(dat = .$counts, mvmin = 1, mvmax = 2))
# 
# test3 <- CompareMixMods(dat = test2$counts[[1]], mvmin = 1, mvmax = 3)

# try parallel
cl <- 5
group <- rep(1:cl, length.out = nrow(params))
params <- bind_cols(tibble(group), params)

cluster <- create_cluster(cores = cl)
by_group <- params[901:905, ] %>%
    partition(group, cluster = cluster)
by_group

mvmin <- 1
mvmax <- 3

# Utilize pipe (%>%) to assign libraries, functions, and values to clusters
by_group %>%
    # Assign libraries
    cluster_library("dplyr") %>%
    cluster_library("tidyr") %>%
    cluster_library("purrr") %>%
    cluster_library("mclust") %>%
    cluster_library("mixsmsn") %>%
    # Assign values (use this to load functions or data to each core)
    cluster_assign_value("gdd", gdd) %>%
    cluster_assign_value("Simulate_Counts", Simulate_Counts) %>%
    cluster_assign_value("CompareMixMods", CompareMixMods) %>% 
      cluster_assign_value("Abund_Curve", Abund_Curve) %>% 
    cluster_assign_value("mvmin", mvmin) %>% 
    cluster_assign_value("mvmax", mvmax)



start <- proc.time() # Start clock
data_par <- by_group %>% # Use by_group party_df
    do(counts = Simulate_Counts(data = ., gdd = gdd)) %>% 
    # mutate(counts = purrr::map(data, ~Simulate_Counts(data = ., gdd = gdd))) %>%
    # mutate(ests = purrr::map(counts, ~CompareMixMods(dat = ., 
                                                     # mvmin = mvmin, mvmax = mvmax))) %>% 
    collect() %>% # Special collect() function to recombine partitions
    as_tibble()   # Convert to tibble
time_elapsed_parallel <- proc.time() - start # End clock

by_group <- data_par %>%
    partition(group, cluster = cluster)
by_group

mvmin <- 1
mvmax <- 3

# Utilize pipe (%>%) to assign libraries, functions, and values to clusters
by_group %>%
    # Assign libraries
    cluster_library("dplyr") %>%
    cluster_library("tidyr") %>%
    cluster_library("purrr") %>%
    cluster_library("mclust") %>%
    cluster_library("mixsmsn") %>%
    # Assign values (use this to load functions or data to each core)
    cluster_assign_value("gdd", gdd) %>%
    cluster_assign_value("Simulate_Counts", Simulate_Counts) %>%
    cluster_assign_value("CompareMixMods", CompareMixMods) %>% 
      cluster_assign_value("Abund_Curve", Abund_Curve) %>% 
    cluster_assign_value("mvmin", mvmin) %>% 
    cluster_assign_value("mvmax", mvmax)



data_par2 <- by_group %>% 
  # group_by(group) %>% 
    do(ests = CompareMixMods(dat = .$counts, mvmin = 1, mvmax = 2)) %>% 
    collect() %>% 
    as_tibble()

test <- CompareMixMods(dat = data_par$counts[[1]], mvmin, mvmax)
test2 <- CompareMixMods(dat = data_par$counts[[4]], mvmin, mvmax)




counts <- data_par$counts[[4]]
counts$SiteID <- as.factor(counts$SiteID)

modnb <- gam(Y ~ 
                         # s(zlistlength)+
                         s(maxT)+
                         # s(zduration)+
                         # s(SiteYear, cumdegday, bs = "fs", k = 5, m = 1) +
                          s(AccumDD, bs = "cc", k = 20) +
                         # te(lat, lon, AccumDD, bs = c("tp", "cc"), k = c(5, 20), d = c(2, 1)) +
                         s(SiteID, bs = "re"),
                         # s(Ordinal, bs = "cc", k = 10),
                       # family = nb(theta = NULL, link = "log"),
                       family = poisson(link = "log"),
                       data = counts,
                       method = "REML", 
                       optimizer = c("outer", "newton"), 
                       # gamma = 1.4, 
                       control = list(maxit = 500))

  counts$Y <- predict(modnb, newdata = counts, type = "response")

test4 <- CompareMixMods(dat = counts, mvmin, mvmax)


```


